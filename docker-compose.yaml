services:
  # --- 1. STORAGE ---
  minio:
    container_name: minio
    image: minio/minio:latest
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    ports:
      - "9000:9000" # S3 API endpoint
      - "9001:9001" # MinIO Console UI
    command: server /data --console-address ":9001"
    volumes:
      - minio_:/data
    restart: unless-stopped

  minio-init:
    image: curlimages/curl:latest
    depends_on:
      minio:
        condition: service_started
    command: >
      /bin/sh -c "
        sleep 5 &&
        echo 'Initializing MinIO bucket...' &&
        curl -X PUT http://minio:9000/mlflow-bucket -u minio:minio123
      "
    restart: "no"

  # --- 2. STREAMING ---
  zookeeper:
    container_name: zookeeper
    image: confluentinc/cp-zookeeper:7.4.12
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    restart: unless-stopped

  kafka:
    container_name: kafka
    image: confluentinc/cp-kafka:7.4.12
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 29092 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s # Allow more time for Kafka to start

  kafka-ui: # NEW: Kafka Web UI for monitoring data flow
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    depends_on:
      - kafka
    ports:
      - "8085:8080" # Exposed on port 8085
    environment:
      KAFKA_CLUSTERS_0_NAME: LocalKafka
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    restart: unless-stopped

  # --- 3. METADATA / TRACKING (Postgres for MLflow & Airflow) ---
  mlflow-db:
    image: postgres:16
    container_name: mlflow-db
    ports:
      - "5433:5432" # Using a different host port (5433) for MLflow DB
    environment:
      POSTGRES_USER: mlflow
      POSTGRES_PASSWORD: changeme
      POSTGRES_DB: mlflow_db
    volumes:
      - mlflow_db:/var/lib/postgresql/data # New volume for MLflow data to avoid compatibility issues
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U mlflow -d mlflow_db"]
      interval: 5s
      timeout: 5s
      retries: 10
    cpus: '0.5'           # Limits CPU to half a core (0.5)
    mem_limit: 512m       # Limits RAM to 512 Megabytes

  airflow-db:
    image: postgres:16
    container_name: airflow-db
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow_db:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 5s
      timeout: 5s
      retries: 10
    restart: unless-stopped
    cpus: '0.5'           # Limits CPU to half a core (0.5)
    mem_limit: 512m       # Limits RAM to 512 Megabytes

  # --- 3. SPARK PROCESSING ---
  spark-master:
    container_name: spark-master
    build:
      context: .
      dockerfile: Dockerfile.spark
    user: root
    environment:
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_WEBUI_PORT: 8080
      SPARK_DAEMON_MEMORY: 512m
      # MinIO credentials (for S3A)
      # Spark Configuration for MinIO (S3A)
      SPARK_CONF_DIR: /opt/spark/conf
      SPARK_CLASSPATH: /opt/spark/jars/*
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio123
      AWS_REGION: us-east-1
    ports:
      - "8081:8080" # Spark Master UI
      - "7077:7077" # Spark Master internal port
    volumes:
      - ./spark_apps:/opt/spark_apps
      - spark-logs:/tmp/spark-events # Shared volume for logs
      - ./spark_config/core-site.xml:/opt/spark/conf/core-site.xml
      - ./jars_3.4.1:/opt/spark/jars
      - ./utils:/opt/spark/utils
    command: >
      bash -c "
        pip install numpy pandas scipy scikit-learn xgboost lightgbm mlflow boto3 joblib pyarrow &&
        /opt/spark/sbin/start-master.sh &&
        tail -f /opt/spark/logs/spark--org.apache.spark.deploy.master.Master-*.out
      "
    restart: unless-stopped
    cpus: '0.5'        # Limit to half a CPU core
    mem_limit: 768m     # Limit to 768MB of RAM

  spark-history: # NEW: Spark History Server
    container_name: spark-history
    build:
      context: .
      dockerfile: Dockerfile.spark
    depends_on:
      - spark-master
    ports:
      - "18080:18080" # Spark History UI
    environment:
      # Location where workers/master write logs (must match SPARK_EVENT_LOG_DIR)
      SPARK_DAEMON_JAVA_OPTS: "-Dspark.history.fs.logDirectory=/tmp/spark-events"
      SPARK_NO_DAEMON_TOOLS: "true" # Prevent starting Master/Worker
    volumes:
      - spark-logs:/tmp/spark-events # Read logs from shared volume
    command: /opt/spark/sbin/start-history-server.sh
    restart: unless-stopped

  spark-worker-1:
    container_name: spark-worker-1
    build:
      context: .
      dockerfile: Dockerfile.spark
    user: root
    depends_on:
      - spark-master
    environment:
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 1g
      SPARK_EVENT_LOG_DIR: /tmp/spark-events # Explicitly set log dir
      SPARK_EVENT_LOG_ENABLED: "true"
      SPARK_CONF_DIR: /opt/spark/conf
      SPARK_CLASSPATH: /opt/spark/jars/*
    volumes:
      - ./spark_apps:/opt/spark_apps
      - spark-logs:/tmp/spark-events # Shared volume for logs
      - ./spark_config/core-site.xml:/opt/spark/conf/core-site.xml
      - ./jars_3.4.1:/opt/spark/jars
      - ./utils:/opt/spark/utils
    command: >
      bash -c "
        export SPARK_CLASSPATH='/opt/spark/jars/*' &&
        pip install numpy pandas scipy scikit-learn xgboost lightgbm mlflow boto3 joblib pyarrow &&
        /opt/spark/sbin/start-worker.sh spark://spark-master:7077 &&
        tail -f /opt/spark/logs/spark--org.apache.spark.deploy.worker.Worker-*.out
      "
    restart: unless-stopped
    cpus: '1.0'        # Limit to one full CPU core
    mem_limit: 1g

  spark-worker-2:
    container_name: spark-worker-2
    build:
      context: .
      dockerfile: Dockerfile.spark
    user: root
    depends_on:
      - spark-master
    environment:
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 1g
      SPARK_EVENT_LOG_DIR: /tmp/spark-events # Explicitly set log dir
      SPARK_EVENT_LOG_ENABLED: "true"
      SPARK_CONF_DIR: /opt/spark/conf
      SPARK_CLASSPATH: /opt/spark/jars/*
    volumes:
      - ./spark_apps:/opt/spark_apps
      - spark-logs:/tmp/spark-events # Shared volume for logs
      - ./spark_config/core-site.xml:/opt/spark/conf/core-site.xml
      - ./jars_3.4.1:/opt/spark/jars
      - ./utils:/opt/spark/utils
    command: >
      bash -c "
        export SPARK_CLASSPATH='/opt/spark/jars/*' &&
        pip install numpy pandas scipy scikit-learn xgboost lightgbm mlflow boto3 joblib pyarrow &&
        /opt/spark/sbin/start-worker.sh spark://spark-master:7077 &&
        tail -f /opt/spark/logs/spark--org.apache.spark.deploy.worker.Worker-*.out
      "
    restart: unless-stopped
    cpus: '1.0'        # Limit to one full CPU core
    mem_limit: 1g

  # --- 4. ORCHESTRATION ---
  airflow:
    container_name: airflow
    build:
      context: .
      dockerfile: Dockerfile.airflow2
    # entrypoint: ["/bin/bash"] # Override entrypoint to use bash
    depends_on:
      airflow-db:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      AIRFLOW__WEBSERVER__WEB_SERVER_PORT: 8080

      # ✉️ Email alert settings
      AIRFLOW__EMAIL__EMAIL_BACKEND: airflow.utils.email.send_email_smtp
      AIRFLOW__SMTP__SMTP_HOST: smtp.gmail.com
      AIRFLOW__SMTP__SMTP_STARTTLS: 'True'
      AIRFLOW__SMTP__SMTP_SSL: 'False'
      AIRFLOW__SMTP__SMTP_USER: jasonling5555@gmail.com
      AIRFLOW__SMTP__SMTP_PASSWORD: ${GMAIL_APP_PASSWORD}
      AIRFLOW__SMTP__SMTP_PORT: 587
      AIRFLOW__SMTP__SMTP_MAIL_FROM: jasonling5555@gmail.com
    env_file:
      - .env
    ports:
      - "8088:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./jars_3.4.1:/opt/spark/jars
      - ./spark_apps:/opt/airflow/spark_apps
      - ./utils:/opt/airflow/utils
    # command: ["-c", "airflow db init && if ! airflow users list 2>/dev/null | grep -q 'admin'; then airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com; fi && echo 'Starting Airflow Scheduler...' && airflow scheduler & echo 'Starting Airflow Webserver...' && airflow webserver"]
    restart: unless-stopped

  # --- 7. MLFLOW ---
  mlflow:
    container_name: mlflow
    build:
      context: .
      dockerfile: Dockerfile.mlflow
    ports:
      - "5000:5000"
    environment:
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio123
      MLFLOW_ARTIFACT_ROOT: s3://mlflow-bucket/
      # Tracking Database (Postgres)
      MLFLOW_TRACKING_URI: postgresql+psycopg2://mlflow:changeme@mlflow-db:5432/mlflow_db
    depends_on:
      mlflow-db: # ADDED: Ensure the DB is ready before starting MLflow server
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
    restart: unless-stopped

  # --- 7. API LAYER (FastAPI) ---
  api: # NEW: Lightweight API for platform interaction
    container_name: fastapi-api
    build:
      context: .
      dockerfile: Dockerfile.api
    ports:
      - "8000:8000" # API exposed on port 8000
    volumes:
      - ./api:/app # Map your local 'api' directory to the container
    # Assumes your FastAPI application is in 'api/app.py' with an object 'app'
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  # --- 5. MONITORING ---
  prometheus:
    container_name: prometheus
    image: prom/prometheus:v2.47.0
    ports:
      - "9091:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    restart: unless-stopped

  grafana:
    container_name: grafana
    image: grafana/grafana:10.1.4
    ports:
      - "3000:3000"
    volumes:
      - grafana_:/var/lib/grafana
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_USERS_ALLOW_SIGN_UP: "false"
    restart: unless-stopped

volumes:
  minio_:
  mlflow_db:
  airflow_db:
  spark-logs: # NEW: Shared volume for Spark event logs
  prometheus_:
  grafana_:
