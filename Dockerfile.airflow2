# Base Airflow image
FROM apache/airflow:2.8.2

# Switch to root to install system dependencies
USER root

# Install Java (for PySpark) and build tools (needed for XGBoost)
RUN apt-get update && apt-get install -y \
    openjdk-17-jdk \
    build-essential \
    gfortran \
    curl \
    git \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV PATH="$JAVA_HOME/bin:$PATH"

# Switch back to airflow user
USER airflow

RUN python -m pip install --upgrade pip

# Install Airflow providers first
RUN pip install --no-cache-dir \
    apache-airflow-providers-postgres psycopg2-binary \
    'apache-airflow-providers-amazon[s3]' \
    apache-airflow-providers-apache-spark

# Install Spark + ML libraries separately
RUN pip install --no-cache-dir pyspark==3.4.1 mlflow boto3

# This step is often more stable when isolated, as they require gfortran.

RUN pip install --no-cache-dir \
    numpy \
    scipy

# Install heavy ML libraries last
RUN pip install --no-cache-dir \
    xgboost \
    scikit-learn \
    pandas \
    pyarrow

# Healthcheck
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

# Default command: run webserver + scheduler in one container
CMD ["bash", "-c", "airflow db upgrade && \
                    airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin || true && \
                    airflow webserver & \
                    airflow scheduler"]
